{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 21:08:49.016002: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-05 21:08:49.202937: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-05 21:08:49.215674: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-05 21:08:49.215707: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-05 21:08:49.284690: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-05 21:08:50.265498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 21:08:50.265663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 21:08:50.265675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#Import all the required libraries\n",
    "import os, glob, random\n",
    "import pickle, time, json\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open(\"../../data/image_caption_mapped.json\", \"r\") as r:\n",
    "    data = json.loads(r.read())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image,caption\n",
      "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset and read the text file into a seperate variable\n",
    "\n",
    "text_file = \"../../data/flikr8k/captions.txt\"\n",
    "\n",
    "def load_doc(filename):\n",
    "    with open(filename,\"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "doc = load_doc(text_file)\n",
    "print(doc[:300])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                          ID  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                                Path  \\\n0  ../../data/flikr8k/Images/1000268201_693b08cb0...   \n1  ../../data/flikr8k/Images/1000268201_693b08cb0...   \n2  ../../data/flikr8k/Images/1000268201_693b08cb0...   \n3  ../../data/flikr8k/Images/1000268201_693b08cb0...   \n4  ../../data/flikr8k/Images/1000268201_693b08cb0...   \n\n                                            Captions  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Path</th>\n      <th>Captions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>../../data/flikr8k/Images/1000268201_693b08cb0...</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>../../data/flikr8k/Images/1000268201_693b08cb0...</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>../../data/flikr8k/Images/1000268201_693b08cb0...</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>../../data/flikr8k/Images/1000268201_693b08cb0...</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>../../data/flikr8k/Images/1000268201_693b08cb0...</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_id= [x[\"image\"] for x in data]\n",
    "all_img_path= [x[\"path\"] for x in data]\n",
    "annotations= [x[\"caption\"] for x in data]\n",
    "\n",
    "df = pd.DataFrame(list(zip(all_img_id, all_img_path,annotations)),columns =['ID','Path', 'Captions'])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions present in the dataset: 40455\n",
      "Total images present in the dataset: 40455\n"
     ]
    }
   ],
   "source": [
    "#Create a list which contains all the captions\n",
    "annotations = annotations\n",
    "\n",
    "#add the <start> & <end> token to all those captions as well\n",
    "annotations = [\"<start> \" + x + \" <end>\" for x in annotations]\n",
    "\n",
    "#Create a list which contains all the path to the images\n",
    "all_img_path = all_img_path\n",
    "\n",
    "print(\"Total captions present in the dataset: \" + str(len(annotations)))\n",
    "print(\"Total images present in the dataset: \" + str(len(all_img_path)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#Create the vocabulary & the counter for the captions\n",
    "\n",
    "vocabulary = {x for x in word_tokenize(\" \".join(annotations)) if x not in stopwords}\n",
    "\n",
    "val_count=Counter(x for x in word_tokenize(\" \".join(annotations)) if x not in stopwords)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('<', 161820),\n ('>', 161820),\n ('end', 80962),\n ('start', 80919),\n ('.', 36581),\n ('A', 22667),\n ('dog', 7984),\n ('man', 6829),\n ('Two', 4365),\n ('white', 3876),\n ('black', 3696),\n ('boy', 3442),\n (',', 3232),\n ('woman', 3228),\n ('girl', 3218),\n ('The', 3089),\n ('wearing', 3061),\n ('water', 2778),\n ('red', 2660),\n ('brown', 2475),\n ('people', 2446),\n ('young', 2432),\n ('blue', 2259),\n ('dogs', 2083),\n ('running', 2072),\n ('playing', 2008),\n ('shirt', 1806),\n ('standing', 1786),\n ('ball', 1779),\n ('little', 1625)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualise the top 30 occuring words in the captions\n",
    "\n",
    "val_count.most_common(30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=\"UNK\", filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'num_words': 5000,\n 'filters': '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n 'lower': True,\n 'split': ' ',\n 'char_level': False,\n 'oov_token': 'UNK',\n 'document_count': 0,\n 'word_counts': '{}',\n 'word_docs': '{}',\n 'index_docs': '{}',\n 'index_word': '{}',\n 'word_index': '{}'}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings.\n",
    "\n",
    "tokenizer.fit_on_texts(annotations)\n",
    "\n",
    "config = tokenizer.get_config()\n",
    "\n",
    "word_to_index = config[\"word_index\"]\n",
    "index_to_word = config[\"index_word\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[('<start>', 80910),\n ('<end>', 80910),\n ('a', 62992),\n ('in', 18986),\n ('the', 18419),\n ('on', 10745),\n ('is', 9345),\n ('and', 8862),\n ('dog', 8138),\n ('with', 7765),\n ('man', 7274),\n ('of', 6723),\n ('two', 5642),\n ('white', 3959),\n ('black', 3848),\n ('boy', 3581),\n ('are', 3504),\n ('woman', 3402),\n ('girl', 3328),\n ('to', 3176),\n ('wearing', 3062),\n ('at', 2915),\n ('people', 2883),\n ('water', 2790),\n ('red', 2691),\n ('young', 2630),\n ('brown', 2578),\n ('an', 2432),\n ('his', 2357),\n ('blue', 2279)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a word count of tokenizer to visulize the Top 30 occuring words after text processing\n",
    "counter = Counter(eval(config[\"word_counts\"]))\n",
    "counter.most_common(30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Caption vector is :(40455, 41)\n"
     ]
    }
   ],
   "source": [
    "# Pad each vector to the max_length of the captions ^ store it to a vairable\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(annotations)\n",
    "cap_vector = pad_sequences(sequences)\n",
    "\n",
    "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
